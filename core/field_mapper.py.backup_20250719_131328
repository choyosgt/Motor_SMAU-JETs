# core/field_mapper.py - VERSI√ìN MEJORADA CON APRENDIZAJE DE PATRONES Y VALIDACI√ìN AVANZADA

import pandas as pd
import re
import unicodedata
from typing import Dict, List, Optional, Tuple, Any
from collections import defaultdict, Counter
import logging
import json
import numpy as np
from datetime import datetime
import os
from pathlib import Path

from .dynamic_field_loader import DynamicFieldLoader
from .dynamic_field_definition import DynamicFieldDefinition

logger = logging.getLogger(__name__)

class ContentPatternLearner:
    """Sistema de aprendizaje de patrones de contenido de datos"""
    
    def __init__(self, patterns_file: str = "config/learned_patterns.json"):
        self.patterns_file = patterns_file
        self.learned_patterns = self._load_learned_patterns()
        
    def _load_learned_patterns(self) -> Dict:
        """Carga patrones aprendidos desde archivo"""
        try:
            if os.path.exists(self.patterns_file):
                with open(self.patterns_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            logger.warning(f"Error loading learned patterns: {e}")
        
        return {
            "field_patterns": {},
            "data_type_patterns": {},
            "validation_stats": {},
            "pattern_confidence": {}
        }
    
    def save_learned_patterns(self):
        """Guarda patrones aprendidos en archivo"""
        try:
            os.makedirs(os.path.dirname(self.patterns_file), exist_ok=True)
            with open(self.patterns_file, 'w', encoding='utf-8') as f:
                json.dump(self.learned_patterns, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"Error saving learned patterns: {e}")
    
    def learn_pattern(self, field_type: str, data_samples: pd.Series, user_confirmed: bool = False):
        """Aprende patrones de un tipo de campo espec√≠fico"""
        if field_type not in self.learned_patterns["field_patterns"]:
            self.learned_patterns["field_patterns"][field_type] = {
                "regex_patterns": [],
                "format_patterns": [],
                "value_patterns": [],
                "statistical_patterns": {}
            }
        
        patterns = self._extract_patterns(data_samples)
        
        # Combinar con patrones existentes
        current_patterns = self.learned_patterns["field_patterns"][field_type]
        
        for pattern_type, new_patterns in patterns.items():
            if pattern_type not in current_patterns:
                current_patterns[pattern_type] = []
            
            # A√±adir nuevos patrones √∫nicos
            for pattern in new_patterns:
                if pattern not in current_patterns[pattern_type]:
                    current_patterns[pattern_type].append(pattern)
        
        # Actualizar estad√≠sticas de confianza
        confidence_key = f"{field_type}_confidence"
        if confidence_key not in self.learned_patterns["pattern_confidence"]:
            self.learned_patterns["pattern_confidence"][confidence_key] = 0.5
        
        # Incrementar confianza si fue confirmado por usuario
        if user_confirmed:
            current_conf = self.learned_patterns["pattern_confidence"][confidence_key]
            self.learned_patterns["pattern_confidence"][confidence_key] = min(current_conf + 0.1, 0.95)
        
        self.save_learned_patterns()
    
    def _extract_patterns(self, data_samples: pd.Series) -> Dict:
        """Extrae patrones de los datos"""
        clean_data = data_samples.dropna().astype(str).head(20)
        patterns = {
            "regex_patterns": [],
            "format_patterns": [],
            "value_patterns": [],
            "statistical_patterns": {}
        }
        
        if len(clean_data) == 0:
            return patterns
        
        # Patrones regex comunes
        regex_patterns = self._extract_regex_patterns(clean_data)
        patterns["regex_patterns"] = regex_patterns
        
        # Patrones de formato
        format_patterns = self._extract_format_patterns(clean_data)
        patterns["format_patterns"] = format_patterns
        
        # Patrones estad√≠sticos
        statistical_patterns = self._extract_statistical_patterns(clean_data)
        patterns["statistical_patterns"] = statistical_patterns
        
        return patterns
    
    def _extract_regex_patterns(self, data: pd.Series) -> List[str]:
        """Extrae patrones regex de los datos"""
        patterns = []
        
        for value in data.head(10):  # Analizar primeros 10 valores
            value_str = str(value).strip()
            
            # Generar patrones b√°sicos
            if re.match(r'^\d{1,2}[/\-\.]\d{1,2}[/\-\.]\d{2,4}$', value_str):
                patterns.append(r'^\d{1,2}[/\-\.]\d{1,2}[/\-\.]\d{2,4}$')
            elif re.match(r'^\d{4}[/\-\.]\d{1,2}[/\-\.]\d{1,2}$', value_str):
                patterns.append(r'^\d{4}[/\-\.]\d{1,2}[/\-\.]\d{1,2}$')
            elif re.match(r'^-?\d{1,3}(\.\d{3})*[,\.]\d{2}$', value_str):
                patterns.append(r'^-?\d{1,3}(\.\d{3})*[,\.]\d{2}$')
            elif re.match(r'^-?\d+[,\.]\d+$', value_str):
                patterns.append(r'^-?\d+[,\.]\d+$')
            elif re.match(r'^\d+$', value_str):
                patterns.append(r'^\d+$')
        
        return list(set(patterns))  # Eliminar duplicados
    
    def _extract_format_patterns(self, data: pd.Series) -> List[str]:
        """Extrae patrones de formato de los datos"""
        patterns = []
        
        for value in data.head(10):
            value_str = str(value).strip()
            
            # Crear patr√≥n de formato
            format_pattern = ""
            for char in value_str:
                if char.isdigit():
                    format_pattern += "D"
                elif char.isalpha():
                    format_pattern += "A"
                elif char in ".,":
                    format_pattern += "."
                elif char in "/-":
                    format_pattern += "-"
                else:
                    format_pattern += "S"
            
            if format_pattern and format_pattern not in patterns:
                patterns.append(format_pattern)
        
        return patterns
    
    def _extract_statistical_patterns(self, data: pd.Series) -> Dict:
        """Extrae patrones estad√≠sticos de los datos"""
        patterns = {}
        
        # Longitud promedio
        lengths = [len(str(val)) for val in data]
        patterns["avg_length"] = np.mean(lengths)
        patterns["length_std"] = np.std(lengths)
        
        # Tipos de caracteres
        has_digits = sum(1 for val in data if any(c.isdigit() for c in str(val)))
        has_letters = sum(1 for val in data if any(c.isalpha() for c in str(val)))
        has_special = sum(1 for val in data if any(not c.isalnum() for c in str(val)))
        
        total = len(data)
        patterns["digit_ratio"] = has_digits / total if total > 0 else 0
        patterns["letter_ratio"] = has_letters / total if total > 0 else 0
        patterns["special_ratio"] = has_special / total if total > 0 else 0
        
        return patterns
    
    def calculate_pattern_match_score(self, field_type: str, data_samples: pd.Series) -> float:
        """Calcula score de coincidencia con patrones aprendidos"""
        if field_type not in self.learned_patterns["field_patterns"]:
            return 0.0
        
        learned_patterns = self.learned_patterns["field_patterns"][field_type]
        if not learned_patterns:
            return 0.0
        
        clean_data = data_samples.dropna().astype(str).head(10)
        if len(clean_data) == 0:
            return 0.0
        
        total_score = 0.0
        weight_sum = 0.0
        
        # Verificar patrones regex
        if learned_patterns.get("regex_patterns"):
            regex_score = self._calculate_regex_match_score(clean_data, learned_patterns["regex_patterns"])
            total_score += regex_score * 0.4
            weight_sum += 0.4
        
        # Verificar patrones de formato
        if learned_patterns.get("format_patterns"):
            format_score = self._calculate_format_match_score(clean_data, learned_patterns["format_patterns"])
            total_score += format_score * 0.3
            weight_sum += 0.3
        
        # Verificar patrones estad√≠sticos
        if learned_patterns.get("statistical_patterns"):
            stats_score = self._calculate_statistical_match_score(clean_data, learned_patterns["statistical_patterns"])
            total_score += stats_score * 0.3
            weight_sum += 0.3
        
        final_score = total_score / weight_sum if weight_sum > 0 else 0.0
        
        # Aplicar confianza aprendida
        confidence_key = f"{field_type}_confidence"
        confidence_multiplier = self.learned_patterns["pattern_confidence"].get(confidence_key, 0.5)
        
        return final_score * confidence_multiplier
    
    def _calculate_regex_match_score(self, data: pd.Series, patterns: List[str]) -> float:
        """Calcula score de coincidencia con patrones regex"""
        matches = 0
        total = len(data)
        
        for value in data:
            value_str = str(value).strip()
            for pattern in patterns:
                try:
                    if re.match(pattern, value_str):
                        matches += 1
                        break
                except re.error:
                    continue
        
        return matches / total if total > 0 else 0.0
    
    def _calculate_format_match_score(self, data: pd.Series, patterns: List[str]) -> float:
        """Calcula score de coincidencia con patrones de formato"""
        matches = 0
        total = len(data)
        
        for value in data:
            value_str = str(value).strip()
            value_format = ""
            
            for char in value_str:
                if char.isdigit():
                    value_format += "D"
                elif char.isalpha():
                    value_format += "A"
                elif char in ".,":
                    value_format += "."
                elif char in "/-":
                    value_format += "-"
                else:
                    value_format += "S"
            
            if value_format in patterns:
                matches += 1
        
        return matches / total if total > 0 else 0.0
    
    def _calculate_statistical_match_score(self, data: pd.Series, patterns: Dict) -> float:
        """Calcula score de coincidencia con patrones estad√≠sticos"""
        current_stats = self._extract_statistical_patterns(data)
        
        scores = []
        
        # Comparar longitud promedio
        if "avg_length" in patterns and "avg_length" in current_stats:
            length_diff = abs(patterns["avg_length"] - current_stats["avg_length"])
            length_score = max(0, 1 - (length_diff / 10))  # Normalizar por 10 caracteres
            scores.append(length_score)
        
        # Comparar ratios de tipos de caracteres
        for ratio_type in ["digit_ratio", "letter_ratio", "special_ratio"]:
            if ratio_type in patterns and ratio_type in current_stats:
                ratio_diff = abs(patterns[ratio_type] - current_stats[ratio_type])
                ratio_score = max(0, 1 - ratio_diff)
                scores.append(ratio_score)
        
        return np.mean(scores) if scores else 0.0


class EnhancedFieldMapper:
    """Mapeador de campos mejorado con validaci√≥n avanzada de contenido y aprendizaje de patrones"""
    
    def __init__(self, config_source: str = None):
        self.field_loader = DynamicFieldLoader(config_source)
        self.pattern_learner = ContentPatternLearner()
        
        # Cache para optimizaci√≥n
        self._mapping_cache = {}
        self._normalization_cache = {}
        self._content_analysis_cache = {}
        
        # Estad√≠sticas de mapeo
        self.mapping_stats = {
            'total_mappings_requested': 0,
            'successful_mappings': 0,
            'failed_mappings': 0,
            'cache_hits': 0,
            'content_analysis_used': 0,
            'pattern_learning_used': 0,
            'conflicts_resolved': 0
        }
        
        # Mapa de traducci√≥n b√°sico
        self.translation_map = {
            'date': 'fecha',
            'amount': 'importe',
            'account': 'cuenta',
            'entry': 'asiento',
            'number': 'numero',
            'description': 'descripcion',
            'currency': 'moneda',
            'debit': 'debe',
            'credit': 'haber'
        }
        
        # Validadores especializados por tipo de campo
        self.field_validators = self._initialize_validators()
        
        print(f"‚úì EnhancedFieldMapper initialized with pattern learning")
    
    def _initialize_validators(self) -> Dict:
        """Inicializa validadores especializados para cada tipo de campo"""
        return {
            'journal_id': self._validate_journal_id,
            'journal_id_line_number': self._validate_line_number,
            'effective_date': self._validate_date,
            'entered_date': self._validate_date,
            'amount': self._validate_amount,
            'amount_debit': self._validate_amount,
            'amount_credit': self._validate_amount,
            'amount_currency': self._validate_currency,
            'amount_credit_debit_indicator': self._validate_indicator,
            'fiscal_year': self._validate_year,
            'period': self._validate_period,
            'gl_account_number': self._validate_account_number,
            'je_header_description': self._validate_description,
            'je_line_description': self._validate_description,
            'company_code': self._validate_code,
            'business_unit_code': self._validate_code,
            'document_type': self._validate_code,
            'entered_by': self._validate_user,
            'reversal_indicator': self._validate_boolean,
            'ledger': self._validate_text
        }
    
    def find_field_mapping(self, field_name: str, erp_system: str = None, 
                          sample_data: pd.Series = None) -> Optional[Tuple[str, float]]:
        """
        Encuentra el mapeo de campo m√°s probable usando an√°lisis de contenido avanzado
        y aprendizaje de patrones
        """
        self.mapping_stats['total_mappings_requested'] += 1
        
        # Verificar cache primero
        cache_key = (field_name, erp_system or 'None', str(hash(tuple(sample_data.dropna().head(3).tolist()))) if sample_data is not None else 'None')
        if cache_key in self._mapping_cache:
            self.mapping_stats['cache_hits'] += 1
            return self._mapping_cache[cache_key]
        
        # Si no hay datos de muestra, hacer an√°lisis b√°sico solo con sin√≥nimos
        if sample_data is None or len(sample_data.dropna()) == 0:
            result = self._basic_synonym_matching(field_name, erp_system)
        else:
            result = self._enhanced_content_analysis(field_name, erp_system, sample_data)
        
        # Guardar en cache
        self._mapping_cache[cache_key] = result
        
        return result
    
    def _enhanced_content_analysis(self, field_name: str, erp_system: str, 
                                 sample_data: pd.Series) -> Optional[Tuple[str, float]]:
        """An√°lisis de contenido mejorado con aprendizaje de patrones"""
        print(f"\nüîç ENHANCED ANALYSIS for '{field_name}':")
        print(f"  Sample data: {sample_data.dropna().head(3).tolist()}")
        
        field_definitions = self.field_loader.get_field_definitions()
        all_candidates = []
        
        # Analizar cada tipo de campo contra el contenido
        for field_type, field_def in field_definitions.items():
            # 1. Score de validaci√≥n de contenido especializada
            content_score = self._analyze_content_coherence(field_type, sample_data)
            
            # 2. Score de patrones aprendidos
            pattern_score = self.pattern_learner.calculate_pattern_match_score(field_type, sample_data)
            
            # 3. Score de coincidencia de sin√≥nimos
            synonym_score = self._get_synonym_score(field_name, field_def, erp_system)
            
            # Combinar scores con pesos optimizados
            if content_score > 0.7 or pattern_score > 0.7:
                # Contenido o patrones muy fuertes - priorizarlos
                total_score = content_score * 0.5 + pattern_score * 0.3 + synonym_score * 0.2
            elif content_score > 0.4 or pattern_score > 0.4:
                # Contenido o patrones moderados
                total_score = content_score * 0.4 + pattern_score * 0.3 + synonym_score * 0.3
            else:
                # Contenido d√©bil - m√°s peso a sin√≥nimos
                total_score = content_score * 0.2 + pattern_score * 0.2 + synonym_score * 0.6
            
            if total_score > 0.15:  # Umbral m√≠nimo
                all_candidates.append({
                    'field_type': field_type,
                    'field_name': field_def.name,
                    'total_score': total_score,
                    'content_score': content_score,
                    'pattern_score': pattern_score,
                    'synonym_score': synonym_score
                })
                
                print(f"  ‚Ä¢ {field_type}: content={content_score:.3f}, "
                      f"pattern={pattern_score:.3f}, synonym={synonym_score:.3f}, "
                      f"total={total_score:.3f}")
        
        # Ordenar candidatos por score total
        all_candidates.sort(key=lambda x: x['total_score'], reverse=True)
        
        if not all_candidates:
            print(f"  ‚ùå No candidates found")
            self.mapping_stats['failed_mappings'] += 1
            return None
        
        # Aplicar reglas especiales de correcci√≥n
        result = self._apply_correction_rules(all_candidates, sample_data)
        if result:
            return result
        
        # Verificar conflictos y resolverlos
        best_candidate = all_candidates[0]
        
        if len(all_candidates) > 1:
            second_best = all_candidates[1]
            score_diff = best_candidate['total_score'] - second_best['total_score']
            
            if score_diff < 0.15:  # Scores muy similares
                print(f"  ‚ö†Ô∏è Close match: {best_candidate['field_type']} ({best_candidate['total_score']:.3f}) "
                      f"vs {second_best['field_type']} ({second_best['total_score']:.3f})")
                self.mapping_stats['conflicts_resolved'] += 1
                
                # Resolver conflicto con reglas espec√≠ficas
                resolved = self._resolve_conflict(best_candidate, second_best, sample_data)
                if resolved:
                    best_candidate = resolved
        
        # Retornar el mejor candidato
        winner = best_candidate['field_type']
        confidence = best_candidate['total_score']
        
        print(f"  ‚úÖ WINNER: {winner} (confidence: {confidence:.3f})")
        
        self.mapping_stats['successful_mappings'] += 1
        self.mapping_stats['content_analysis_used'] += 1
        
        return (winner, confidence)
    
    def _apply_correction_rules(self, candidates: List[Dict], sample_data: pd.Series) -> Optional[Tuple[str, float]]:
        """Aplica reglas espec√≠ficas de correcci√≥n para casos problem√°ticos"""
        
        # Regla 1: Si detecta fecha, NO puede ser journal_id
        if self._is_date_data(sample_data):
            date_candidates = [c for c in candidates if 'date' in c['field_type']]
            if date_candidates:
                best_date = max(date_candidates, key=lambda x: x['total_score'])
                print(f"  üîß CORRECTION: Date detected -> {best_date['field_type']}")
                return (best_date['field_type'], best_date['total_score'])
        
        # Regla 2: Si detecta importe, NO puede ser currency o indicator
        if self._is_amount_data(sample_data):
            amount_candidates = [c for c in candidates if c['field_type'] in ['amount', 'amount_debit', 'amount_credit']]
            if amount_candidates:
                best_amount = max(amount_candidates, key=lambda x: x['total_score'])
                print(f"  üîß CORRECTION: Amount detected -> {best_amount['field_type']}")
                return (best_amount['field_type'], best_amount['total_score'])
        
        # Regla 3: Si detecta texto descriptivo, NO puede ser amount
        if self._is_description_data(sample_data):
            desc_candidates = [c for c in candidates if 'description' in c['field_type']]
            if desc_candidates:
                best_desc = max(desc_candidates, key=lambda x: x['total_score'])
                print(f"  üîß CORRECTION: Description detected -> {best_desc['field_type']}")
                return (best_desc['field_type'], best_desc['total_score'])
        
        return None
    
    def _resolve_conflict(self, candidate1: Dict, candidate2: Dict, sample_data: pd.Series) -> Optional[Dict]:
        """Resuelve conflictos entre candidatos con scores similares"""
        
        # Priorizar campos que tengan mejor validaci√≥n de contenido espec√≠fica
        val1 = self._validate_field_content(candidate1['field_type'], sample_data)
        val2 = self._validate_field_content(candidate2['field_type'], sample_data)
        
        if val1 > val2 + 0.2:
            return candidate1
        elif val2 > val1 + 0.2:
            return candidate2
        
        # Si siguen empatados, priorizar por tipo de campo (fechas > importes > textos)
        priority_order = {
            'effective_date': 10, 'entered_date': 9,
            'amount': 8, 'amount_debit': 7, 'amount_credit': 6,
            'journal_id': 5, 'journal_id_line_number': 4,
            'gl_account_number': 3,
            'je_header_description': 2, 'je_line_description': 1
        }
        
        prio1 = priority_order.get(candidate1['field_type'], 0)
        prio2 = priority_order.get(candidate2['field_type'], 0)
        
        return candidate1 if prio1 > prio2 else candidate2
    
    def _analyze_content_coherence(self, field_type: str, sample_data: pd.Series) -> float:
        """
        An√°lisis de coherencia de contenido mejorado con validadores especializados
        """
        if field_type not in self.field_validators:
            return 0.0
        
        # Verificar cache
        cache_key = (field_type, str(hash(tuple(sample_data.dropna().head(5).tolist()))))
        if cache_key in self._content_analysis_cache:
            return self._content_analysis_cache[cache_key]
        
        # Ejecutar validador especializado
        try:
            score = self.field_validators[field_type](sample_data)
            score = max(0.0, min(1.0, score))  # Normalizar entre 0 y 1
            
            # Guardar en cache
            self._content_analysis_cache[cache_key] = score
            
            return score
            
        except Exception as e:
            logger.warning(f"Error in content coherence analysis for {field_type}: {e}")
            return 0.0
    
    def _validate_field_content(self, field_type: str, sample_data: pd.Series) -> float:
        """Validaci√≥n espec√≠fica de contenido por tipo de campo"""
        return self._analyze_content_coherence(field_type, sample_data)
    
    # ====== VALIDADORES ESPECIALIZADOS ======
    
    def _validate_journal_id(self, series: pd.Series) -> float:
        """Valida identificadores de asientos contables"""
        clean_series = series.dropna().astype(str)
        if len(clean_series) == 0:
            return 0.0
        
        valid_count = 0
        total_count = len(clean_series)
        
        for value in clean_series:
            value = str(value).strip()
            
            # Patrones para journal IDs
            patterns = [
                r'^\d{6,15}$',                    # N√∫meros largos
                r'^JE\d{6,12}$',                  # JE + n√∫meros
                r'^AST\d{4,10}$',                 # AST + n√∫meros
                r'^[A-Z]{2,4}\d{6,12}$',         # Letras + n√∫meros
                r'^\d{4}[A-Z]{2,4}\d{4,8}$',     # A√±o + letras + n√∫meros
                r'^[A-Z0-9]{8,20}$'              # Alfanum√©rico general
            ]
            
            # Verificar patrones
            if any(re.match(pattern, value.upper()) for pattern in patterns):
                valid_count += 1
            elif len(value) >= 6 and value.isdigit():
                valid_count += 0.8  # N√∫meros largos probablemente v√°lidos
            elif len(value) >= 4 and re.match(r'^[A-Z0-9]+$', value.upper()):
                valid_count += 0.6  # Alfanum√©rico general
            
            # PENALIZAR si parece fecha
            if self._is_date_like(value):
                valid_count -= 0.5
        
        return max(0.0, min(valid_count / total_count, 1.0))
    
    def _validate_line_number(self, series: pd.Series) -> float:
        """Valida n√∫meros de l√≠nea de asiento"""
        clean_series = series.dropna()
        if len(clean_series) == 0:
            return 0.0
        
        valid_count = 0
        total_count = len(clean_series)
        
        for value in clean_series:
            try:
                num_value = float(str(value).replace(',', '.'))
                if 1 <= num_value <= 9999 and num_value == int(num_value):
                    valid_count += 1
                elif str(value).strip().isdigit():
                    valid_count += 0.8
            except:
                # Si parece fecha, penalizar fuertemente
                if self._is_date_like(str(value)):
                    valid_count -= 0.3
        
        return max(0.0, min(valid_count / total_count, 1.0))
    
    def _validate_date(self, series: pd.Series) -> float:
        """Valida fechas"""
        clean_series = series.dropna().astype(str)
        if len(clean_series) == 0:
            return 0.0
        
        valid_count = 0
        total_count = len(clean_series)
        
        for value in clean_series:
            if self._is_date_like(value):
                valid_count += 1
        
        return valid_count / total_count
    
    def _validate_amount(self, series: pd.Series) -> float:
        """Valida importes monetarios"""
        clean_series = series.dropna().astype(str)
        if len(clean_series) == 0:
            return 0.0
        
        valid_count = 0
        total_count = len(clean_series)
        
        for value in clean_series:
            value_str = str(value).strip()
            
            # Patrones de importes
            amount_patterns = [
                r'^-?\d{1,3}(\.\d{3})*,\d{2}$',      # 1.234.567,89
                r'^-?\d{1,3}(,\d{3})*\.\d{2}$',      # 1,234,567.89
                r'^-?\d+[,\.]\d{1,4}$',              # 1234,56 o 1234.56
                r'^-?\d+$',                          # 1234
                r'^-?\d+\.\d+$',                     # 1234.56
                r'^-?\d+,\d+$'                       # 1234,56
            ]
            
            if any(re.match(pattern, value_str) for pattern in amount_patterns):
                valid_count += 1
            elif self._is_numeric(value_str):
                valid_count += 0.7
            
            # PENALIZAR si parece descripci√≥n
            if len(value_str) > 2 and any(c.isalpha() for c in value_str):
                valid_count -= 0.3
        
        return max(0.0, min(valid_count / total_count, 1.0))
    
    def _validate_currency(self, series: pd.Series) -> float:
        """Valida c√≥digos de moneda"""
        clean_series = series.dropna().astype(str)
        if len(clean_series) == 0:
            return 0.0
        
        valid_count = 0
        total_count = len(clean_series)
        
        common_currencies = {'EUR', 'USD', 'GBP', 'JPY', 'CHF', 'CAD', 'AUD', 'CNY', 'MXN'}
        
        for value in clean_series:
            value_str = str(value).strip().upper()
            
            if value_str in common_currencies:
                valid_count += 1
            elif len(value_str) == 3 and value_str.isalpha():
                valid_count += 0.7
            elif value_str in ['‚Ç¨', '$', '¬£', '¬•']:
                valid_count += 0.8
            
            # PENALIZAR si parece importe num√©rico
            if self._is_numeric(value_str):
                valid_count -= 0.5
        
        return max(0.0, min(valid_count / total_count, 1.0))
    
    def _validate_indicator(self, series: pd.Series) -> float:
        """Valida indicadores debe/haber"""
        clean_series = series.dropna().astype(str)
        if len(clean_series) == 0:
            return 0.0
        
        valid_count = 0
        total_count = len(clean_series)
        
        valid_indicators = {'D', 'C', 'H', 'DEBE', 'HABER', 'DEBIT', 'CREDIT', 'DR', 'CR', '1', '0', '-1'}
        
        for value in clean_series:
            value_str = str(value).strip().upper()
            
            if value_str in valid_indicators:
                valid_count += 1
            elif len(value_str) == 1 and value_str in 'DCHX+-':
                valid_count += 0.8
            
            # PENALIZAR fuertemente si parece importe num√©rico grande
            if self._is_numeric(value_str) and abs(float(value_str.replace(',', '.'))) > 10:
                valid_count -= 0.8
        
        return max(0.0, min(valid_count / total_count, 1.0))
    
    def _validate_year(self, series: pd.Series) -> float:
        """Valida a√±os fiscales"""
        clean_series = series.dropna()
        if len(clean_series) == 0:
            return 0.0
        
        valid_count = 0
        total_count = len(clean_series)
        
        for value in clean_series:
            try:
                year_value = int(float(str(value)))
                if 1900 <= year_value <= 2100:
                    valid_count += 1
                elif 10 <= year_value <= 99:  # A√±os en formato de 2 d√≠gitos
                    valid_count += 0.7
            except:
                pass
        
        return valid_count / total_count
    
    def _validate_period(self, series: pd.Series) -> float:
        """Valida per√≠odos contables"""
        clean_series = series.dropna()
        if len(clean_series) == 0:
            return 0.0
        
        valid_count = 0
        total_count = len(clean_series)
        
        for value in clean_series:
            try:
                period_value = int(float(str(value)))
                if 1 <= period_value <= 12:
                    valid_count += 1
                elif 1 <= period_value <= 53:  # Semanas
                    valid_count += 0.6
            except:
                value_str = str(value).upper()
                # Per√≠odos en formato texto
                if any(month in value_str for month in ['ENE', 'FEB', 'MAR', 'ABR', 'MAY', 'JUN',
                                                      'JUL', 'AGO', 'SEP', 'OCT', 'NOV', 'DIC']):
                    valid_count += 0.8
        
        return valid_count / total_count
    
    def _validate_account_number(self, series: pd.Series) -> float:
        """Valida n√∫meros de cuenta contable"""
        clean_series = series.dropna().astype(str)
        if len(clean_series) == 0:
            return 0.0
        
        valid_count = 0
        total_count = len(clean_series)
        
        for value in clean_series:
            value_str = str(value).strip()
            
            # Patrones de cuentas contables
            if re.match(r'^\d{3,10}$', value_str):  # Solo n√∫meros, 3-10 d√≠gitos
                valid_count += 1
            elif re.match(r'^\d{3,6}\.\d{2,4}$', value_str):  # Con punto
                valid_count += 1
            elif re.match(r'^\d{3,6}-\d{2,4}$', value_str):  # Con gui√≥n
                valid_count += 1
            elif len(value_str) >= 3 and value_str.isdigit():
                valid_count += 0.8
            
            # PENALIZAR si parece per√≠odo (n√∫meros muy peque√±os)
            try:
                num_val = int(value_str)
                if 1 <= num_val <= 12:
                    valid_count -= 0.3
            except:
                pass
        
        return max(0.0, min(valid_count / total_count, 1.0))
    
    def _validate_description(self, series: pd.Series) -> float:
        """Valida descripciones de texto"""
        clean_series = series.dropna().astype(str)
        if len(clean_series) == 0:
            return 0.0
        
        valid_count = 0
        total_count = len(clean_series)
        
        for value in clean_series:
            value_str = str(value).strip()
            
            # Debe contener texto y tener longitud razonable
            if len(value_str) >= 3:
                has_letters = any(c.isalpha() for c in value_str)
                has_spaces_or_words = ' ' in value_str or len(value_str) > 10
                
                if has_letters and has_spaces_or_words:
                    valid_count += 1
                elif has_letters:
                    valid_count += 0.6
            
            # PENALIZAR fuertemente si es completamente num√©rico
            if value_str.replace('.', '').replace(',', '').replace('-', '').isdigit():
                valid_count -= 0.5
        
        return max(0.0, min(valid_count / total_count, 1.0))
    
    def _validate_code(self, series: pd.Series) -> float:
        """Valida c√≥digos (empresa, unidad de negocio, documento)"""
        clean_series = series.dropna().astype(str)
        if len(clean_series) == 0:
            return 0.0
        
        valid_count = 0
        total_count = len(clean_series)
        
        for value in clean_series:
            value_str = str(value).strip()
            
            # C√≥digos suelen ser cortos y alfanum√©ricos
            if 2 <= len(value_str) <= 10:
                if re.match(r'^[A-Z0-9]+$', value_str.upper()):
                    valid_count += 1
                elif value_str.replace('-', '').replace('_', '').isalnum():
                    valid_count += 0.8
        
        return valid_count / total_count
    
    def _validate_user(self, series: pd.Series) -> float:
        """Valida nombres de usuario"""
        clean_series = series.dropna().astype(str)
        if len(clean_series) == 0:
            return 0.0
        
        valid_count = 0
        total_count = len(clean_series)
        
        for value in clean_series:
            value_str = str(value).strip()
            
            # Usuarios suelen ser c√≥digos o nombres
            if len(value_str) >= 2:
                if re.match(r'^[A-Za-z][A-Za-z0-9._-]*$', value_str):
                    valid_count += 1
                elif any(c.isalpha() for c in value_str):
                    valid_count += 0.6
        
        return valid_count / total_count
    
    def _validate_boolean(self, series: pd.Series) -> float:
        """Valida indicadores booleanos"""
        clean_series = series.dropna().astype(str)
        if len(clean_series) == 0:
            return 0.0
        
        valid_count = 0
        total_count = len(clean_series)
        
        boolean_values = {'Y', 'N', 'S', 'TRUE', 'FALSE', '1', '0', 'SI', 'NO', 'YES'}
        
        for value in clean_series:
            value_str = str(value).strip().upper()
            
            if value_str in boolean_values:
                valid_count += 1
            elif len(value_str) == 1 and value_str in 'YNSTF10':
                valid_count += 0.8
        
        return valid_count / total_count
    
    def _validate_text(self, series: pd.Series) -> float:
        """Validaci√≥n gen√©rica para campos de texto"""
        clean_series = series.dropna().astype(str)
        if len(clean_series) == 0:
            return 0.0
        
        valid_count = 0
        total_count = len(clean_series)
        
        for value in clean_series:
            value_str = str(value).strip()
            
            if len(value_str) >= 1:
                valid_count += 1
        
        return valid_count / total_count
    
    # ====== M√âTODOS DE UTILIDAD ======
    
    def _is_date_data(self, series: pd.Series) -> bool:
        """Verifica si los datos parecen fechas"""
        clean_series = series.dropna().astype(str).head(5)
        date_count = sum(1 for value in clean_series if self._is_date_like(value))
        return date_count / len(clean_series) > 0.6 if len(clean_series) > 0 else False
    
    def _is_amount_data(self, series: pd.Series) -> bool:
        """Verifica si los datos parecen importes"""
        clean_series = series.dropna().astype(str).head(5)
        amount_count = sum(1 for value in clean_series if self._is_numeric(value) and not self._is_date_like(value))
        return amount_count / len(clean_series) > 0.6 if len(clean_series) > 0 else False
    
    def _is_description_data(self, series: pd.Series) -> bool:
        """Verifica si los datos parecen descripciones"""
        clean_series = series.dropna().astype(str).head(5)
        desc_count = 0
        
        for value in clean_series:
            if len(value) > 5 and any(c.isalpha() for c in value):
                desc_count += 1
        
        return desc_count / len(clean_series) > 0.6 if len(clean_series) > 0 else False
    
    def _is_date_like(self, value: str) -> bool:
        """Verifica si un valor parece una fecha"""
        value_str = str(value).strip()
        
        date_patterns = [
            r'^\d{1,2}[/\-\.]\d{1,2}[/\-\.]\d{2,4}$',
            r'^\d{4}[/\-\.]\d{1,2}[/\-\.]\d{1,2}$',
            r'^\d{1,2}-[a-zA-Z]{3}-\d{2,4}$',
            r'^\d{2,4}[/\-\.]\d{1,2}[/\-\.]\d{1,2}$'
        ]
        
        return any(re.match(pattern, value_str) for pattern in date_patterns)
    
    def _is_numeric(self, value: str) -> bool:
        """Verifica si un valor es num√©rico"""
        try:
            # Limpiar formato de n√∫mero
            clean_value = str(value).replace(',', '.').replace(' ', '')
            # Permitir un signo negativo al inicio
            if clean_value.startswith('-'):
                clean_value = clean_value[1:]
            
            float(clean_value)
            return True
        except:
            return False
    
    def _get_synonym_score(self, field_name: str, field_def: DynamicFieldDefinition, 
                          erp_system: str = None) -> float:
        """Calcula score de coincidencia con sin√≥nimos"""
        # Implementaci√≥n b√°sica de sin√≥nimos (mantenida del c√≥digo original)
        normalized_name = self._normalize_field_name(field_name)
        
        if not hasattr(field_def, 'synonyms_by_erp'):
            return 0.0
        
        best_score = 0.0
        
        # Buscar en sin√≥nimos espec√≠ficos del ERP
        if erp_system and erp_system in field_def.synonyms_by_erp:
            for synonym_data in field_def.synonyms_by_erp[erp_system]:
                synonym_normalized = self._normalize_field_name(synonym_data.name)
                
                if normalized_name == synonym_normalized:
                    score = 0.9 * synonym_data.confidence_boost
                    best_score = max(best_score, score)
                elif synonym_normalized in normalized_name or normalized_name in synonym_normalized:
                    score = 0.7 * synonym_data.confidence_boost
                    best_score = max(best_score, score)
        
        # Buscar en otros ERPs si no se encontr√≥ en el espec√≠fico
        if best_score < 0.3:
            for erp, synonyms in field_def.synonyms_by_erp.items():
                if erp == erp_system:
                    continue
                    
                for synonym_data in synonyms:
                    synonym_normalized = self._normalize_field_name(synonym_data.name)
                    
                    if normalized_name == synonym_normalized:
                        score = 0.8 * synonym_data.confidence_boost
                        best_score = max(best_score, score)
                    elif synonym_normalized in normalized_name or normalized_name in synonym_normalized:
                        score = 0.6 * synonym_data.confidence_boost
                        best_score = max(best_score, score)
        
        return best_score
    
    def _normalize_field_name(self, field_name: str) -> str:
        """Normaliza nombres de campos para comparaci√≥n"""
        if field_name in self._normalization_cache:
            return self._normalization_cache[field_name]
        
        # Proceso de normalizaci√≥n
        normalized = field_name.lower().strip()
        
        # Eliminar acentos
        normalized = unicodedata.normalize('NFD', normalized)
        normalized = ''.join(c for c in normalized if unicodedata.category(c) != 'Mn')
        
        # Eliminar caracteres especiales
        normalized = re.sub(r'[^a-z0-9\s]', ' ', normalized)
        
        # Reemplazar m√∫ltiples espacios por uno solo
        normalized = re.sub(r'\s+', ' ', normalized).strip()
        
        # Eliminar espacios para comparaci√≥n exacta
        normalized = normalized.replace(' ', '')
        
        # Traducir t√©rminos comunes
        for foreign_word, spanish_word in self.translation_map.items():
            if foreign_word in normalized:
                normalized = normalized.replace(foreign_word, spanish_word)
        
        self._normalization_cache[field_name] = normalized
        return normalized
    
    def _basic_synonym_matching(self, field_name: str, erp_system: str = None) -> Optional[Tuple[str, float]]:
        """An√°lisis b√°sico basado solo en sin√≥nimos (fallback)"""
        normalized_name = self._normalize_field_name(field_name)
        best_match = None
        best_confidence = 0.0
        
        field_definitions = self.field_loader.get_field_definitions()
        
        for field_type, field_def in field_definitions.items():
            confidence = self._get_synonym_score(field_name, field_def, erp_system)
            
            if confidence > best_confidence:
                best_confidence = confidence
                best_match = field_type
        
        if best_match and best_confidence > 0.3:
            self.mapping_stats['successful_mappings'] += 1
            return (best_match, best_confidence)
        
        self.mapping_stats['failed_mappings'] += 1
        return None
    
    def learn_from_training(self, field_type: str, field_name: str, sample_data: pd.Series, 
                           user_confirmed: bool = True):
        """Aprende patrones de un entrenamiento espec√≠fico"""
        if sample_data is not None and len(sample_data.dropna()) > 0:
            self.pattern_learner.learn_pattern(field_type, sample_data, user_confirmed)
            self.mapping_stats['pattern_learning_used'] += 1
            print(f"  üìö Pattern learned for {field_type} from '{field_name}'")
    
    def get_mapping_statistics(self) -> Dict:
        """Retorna estad√≠sticas de mapeo"""
        stats = self.mapping_stats.copy()
        stats['success_rate'] = stats['successful_mappings'] / max(stats['total_mappings_requested'], 1) * 100
        stats['cache_hit_rate'] = stats['cache_hits'] / max(stats['total_mappings_requested'], 1) * 100
        return stats
    
    def clear_cache(self):
        """Limpia caches para liberar memoria"""
        self._mapping_cache.clear()
        self._normalization_cache.clear()
        self._content_analysis_cache.clear()
        print("‚úì Field mapper caches cleared")


# Mantener compatibilidad con el c√≥digo existente
FieldMapper = EnhancedFieldMapper