# core/field_mapper.py - VERSIÓN MEJORADA CON APRENDIZAJE DE PATRONES Y VALIDACIÓN AVANZADA

import pandas as pd
import re
import unicodedata
from typing import Dict, List, Optional, Tuple, Any
from collections import defaultdict, Counter
import logging
import json
import numpy as np
from datetime import datetime
import os
from pathlib import Path

from .dynamic_field_loader import DynamicFieldLoader
from .dynamic_field_definition import DynamicFieldDefinition

logger = logging.getLogger(__name__)

class ContentPatternLearner:
    """Sistema de aprendizaje de patrones de contenido de datos"""
    
    def __init__(self, patterns_file: str = "config/learned_patterns.json"):
        self.patterns_file = patterns_file
        self.learned_patterns = self._load_learned_patterns()
        
    def _load_learned_patterns(self) -> Dict:
        """Carga patrones aprendidos desde archivo"""
        try:
            if os.path.exists(self.patterns_file):
                with open(self.patterns_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            logger.warning(f"Error loading learned patterns: {e}")
        
        return {
            "field_patterns": {},
            "data_type_patterns": {},
            "validation_stats": {},
            "pattern_confidence": {}
        }
    
    def save_learned_patterns(self):
        """Guarda patrones aprendidos en archivo"""
        try:
            os.makedirs(os.path.dirname(self.patterns_file), exist_ok=True)
            with open(self.patterns_file, 'w', encoding='utf-8') as f:
                json.dump(self.learned_patterns, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"Error saving learned patterns: {e}")
    
    def learn_pattern(self, field_type: str, data_samples: pd.Series, user_confirmed: bool = False):
        """Aprende patrones de un tipo de campo específico"""
        if field_type not in self.learned_patterns["field_patterns"]:
            self.learned_patterns["field_patterns"][field_type] = {
                "regex_patterns": [],
                "format_patterns": [],
                "value_patterns": [],
                "statistical_patterns": {}
            }
        
        patterns = self._extract_patterns(data_samples)
        
        # Combinar con patrones existentes
        current_patterns = self.learned_patterns["field_patterns"][field_type]
        
        for pattern_type, new_patterns in patterns.items():
            if pattern_type not in current_patterns:
                current_patterns[pattern_type] = []
            
            # Añadir nuevos patrones únicos
            for pattern in new_patterns:
                if pattern not in current_patterns[pattern_type]:
                    current_patterns[pattern_type].append(pattern)
        
        # Actualizar estadísticas de confianza
        confidence_key = f"{field_type}_confidence"
        if confidence_key not in self.learned_patterns["pattern_confidence"]:
            self.learned_patterns["pattern_confidence"][confidence_key] = 0.5
        
        # Incrementar confianza si fue confirmado por usuario
        if user_confirmed:
            current_conf = self.learned_patterns["pattern_confidence"][confidence_key]
            self.learned_patterns["pattern_confidence"][confidence_key] = min(current_conf + 0.1, 0.95)
        
        self.save_learned_patterns()
    
    def _extract_patterns(self, data_samples: pd.Series) -> Dict:
        """Extrae patrones de los datos"""
        clean_data = data_samples.dropna().astype(str).head(20)
        patterns = {
            "regex_patterns": [],
            "format_patterns": [],
            "value_patterns": [],
            "statistical_patterns": {}
        }
        
        if len(clean_data) == 0:
            return patterns
        
        # Patrones regex comunes
        regex_patterns = self._extract_regex_patterns(clean_data)
        patterns["regex_patterns"] = regex_patterns
        
        # Patrones de formato
        format_patterns = self._extract_format_patterns(clean_data)
        patterns["format_patterns"] = format_patterns
        
        # Patrones estadísticos
        statistical_patterns = self._extract_statistical_patterns(clean_data)
        patterns["statistical_patterns"] = statistical_patterns
        
        return patterns
    
    def _extract_regex_patterns(self, data: pd.Series) -> List[str]:
        """Extrae patrones regex de los datos"""
        patterns = []
        
        for value in data.head(10):  # Analizar primeros 10 valores
            value_str = str(value).strip()
            
            # Generar patrones básicos
            if re.match(r'^\d{1,2}[/\-\.]\d{1,2}[/\-\.]\d{2,4}$', value_str):
                patterns.append(r'^\d{1,2}[/\-\.]\d{1,2}[/\-\.]\d{2,4}$')
            elif re.match(r'^\d{4}[/\-\.]\d{1,2}[/\-\.]\d{1,2}$', value_str):
                patterns.append(r'^\d{4}[/\-\.]\d{1,2}[/\-\.]\d{1,2}$')
            elif re.match(r'^-?\d{1,3}(\.\d{3})*[,\.]\d{2}$', value_str):
                patterns.append(r'^-?\d{1,3}(\.\d{3})*[,\.]\d{2}$')
            elif re.match(r'^-?\d+[,\.]\d+$', value_str):
                patterns.append(r'^-?\d+[,\.]\d+$')
            elif re.match(r'^\d+$', value_str):
                patterns.append(r'^\d+$')
        
        return list(set(patterns))  # Eliminar duplicados
    
    def _extract_format_patterns(self, data: pd.Series) -> List[str]:
        """Extrae patrones de formato de los datos"""
        patterns = []
        
        for value in data.head(10):
            value_str = str(value).strip()
            
            # Crear patrón de formato
            format_pattern = ""
            for char in value_str:
                if char.isdigit():
                    format_pattern += "D"
                elif char.isalpha():
                    format_pattern += "A"
                elif char in ".,":
                    format_pattern += "."
                elif char in "/-":
                    format_pattern += "-"
                else:
                    format_pattern += "S"
            
            if format_pattern and format_pattern not in patterns:
                patterns.append(format_pattern)
        
        return patterns
    
    def _extract_statistical_patterns(self, data: pd.Series) -> Dict:
        """Extrae patrones estadísticos de los datos"""
        patterns = {}
        
        # Longitud promedio
        lengths = [len(str(val)) for val in data]
        patterns["avg_length"] = np.mean(lengths)
        patterns["length_std"] = np.std(lengths)
        
        # Tipos de caracteres
        has_digits = sum(1 for val in data if any(c.isdigit() for c in str(val)))
        has_letters = sum(1 for val in data if any(c.isalpha() for c in str(val)))
        has_special = sum(1 for val in data if any(not c.isalnum() for c in str(val)))
        
        total = len(data)
        patterns["digit_ratio"] = has_digits / total if total > 0 else 0
        patterns["letter_ratio"] = has_letters / total if total > 0 else 0
        patterns["special_ratio"] = has_special / total if total > 0 else 0
        
        return patterns
    
    def calculate_pattern_match_score(self, field_type: str, data_samples: pd.Series) -> float:
        """Calcula score de coincidencia con patrones aprendidos"""
        if field_type not in self.learned_patterns["field_patterns"]:
            return 0.0
        
        learned_patterns = self.learned_patterns["field_patterns"][field_type]
        if not learned_patterns:
            return 0.0
        
        clean_data = data_samples.dropna().astype(str).head(10)
        if len(clean_data) == 0:
            return 0.0
        
        total_score = 0.0
        weight_sum = 0.0
        
        # Verificar patrones regex
        if learned_patterns.get("regex_patterns"):
            regex_score = self._calculate_regex_match_score(clean_data, learned_patterns["regex_patterns"])
            total_score += regex_score * 0.4
            weight_sum += 0.4
        
        # Verificar patrones de formato
        if learned_patterns.get("format_patterns"):
            format_score = self._calculate_format_match_score(clean_data, learned_patterns["format_patterns"])
            total_score += format_score * 0.3
            weight_sum += 0.3
        
        # Verificar patrones estadísticos
        if learned_patterns.get("statistical_patterns"):
            stats_score = self._calculate_statistical_match_score(clean_data, learned_patterns["statistical_patterns"])
            total_score += stats_score * 0.3
            weight_sum += 0.3
        
        final_score = total_score / weight_sum if weight_sum > 0 else 0.0
        
        # Aplicar confianza aprendida
        confidence_key = f"{field_type}_confidence"
        confidence_multiplier = self.learned_patterns["pattern_confidence"].get(confidence_key, 0.5)
        
        return final_score * confidence_multiplier
    
    def _calculate_regex_match_score(self, data: pd.Series, patterns: List[str]) -> float:
        """Calcula score de coincidencia con patrones regex"""
        matches = 0
        total = len(data)
        
        for value in data:
            value_str = str(value).strip()
            for pattern in patterns:
                try:
                    if re.match(pattern, value_str):
                        matches += 1
                        break
                except re.error:
                    continue
        
        return matches / total if total > 0 else 0.0
    
    def _calculate_format_match_score(self, data: pd.Series, patterns: List[str]) -> float:
        """Calcula score de coincidencia con patrones de formato"""
        matches = 0
        total = len(data)
        
        for value in data:
            value_str = str(value).strip()
            value_format = ""
            
            for char in value_str:
                if char.isdigit():
                    value_format += "D"
                elif char.isalpha():
                    value_format += "A"
                elif char in ".,":
                    value_format += "."
                elif char in "/-":
                    value_format += "-"
                else:
                    value_format += "S"
            
            if value_format in patterns:
                matches += 1
        
        return matches / total if total > 0 else 0.0
    
    def _calculate_statistical_match_score(self, data: pd.Series, patterns: Dict) -> float:
        """Calcula score de coincidencia con patrones estadísticos"""
        current_stats = self._extract_statistical_patterns(data)
        
        scores = []
        
        # Comparar longitud promedio
        if "avg_length" in patterns and "avg_length" in current_stats:
            length_diff = abs(patterns["avg_length"] - current_stats["avg_length"])
            length_score = max(0, 1 - (length_diff / 10))  # Normalizar por 10 caracteres
            scores.append(length_score)
        
        # Comparar ratios de tipos de caracteres
        for ratio_type in ["digit_ratio", "letter_ratio", "special_ratio"]:
            if ratio_type in patterns and ratio_type in current_stats:
                ratio_diff = abs(patterns[ratio_type] - current_stats[ratio_type])
                ratio_score = max(0, 1 - ratio_diff)
                scores.append(ratio_score)
        
        return np.mean(scores) if scores else 0.0


class EnhancedFieldMapper:
    """Mapeador de campos mejorado con validación avanzada de contenido y aprendizaje de patrones"""
    
    def __init__(self, config_source: str = None):
        self.field_loader = DynamicFieldLoader(config_source)
        self.pattern_learner = ContentPatternLearner()
        
        # Cache para optimización
        self._mapping_cache = {}
        self._normalization_cache = {}
        self._content_analysis_cache = {}
        
        # Estadísticas de mapeo
        self.mapping_stats = {
            'total_mappings_requested': 0,
            'successful_mappings': 0,
            'failed_mappings': 0,
            'cache_hits': 0,
            'content_analysis_used': 0,
            'pattern_learning_used': 0,
            'conflicts_resolved': 0
        }
        
        # Mapa de traducción básico
        self.translation_map = {
            'date': 'fecha',
            'amount': 'importe',
            'account': 'cuenta',
            'entry': 'asiento',
            'number': 'numero',
            'description': 'descripcion',
            'currency': 'moneda',
            'debit': 'debe',
            'credit': 'haber'
        }
        
        # Validadores especializados por tipo de campo
        self.field_validators = self._initialize_validators()
        
        print(f"✓ EnhancedFieldMapper initialized with pattern learning")
    
    def _initialize_validators(self) -> Dict:
        """Inicializa validadores especializados para cada tipo de campo"""
        return {
            'journal_id': self._validate_journal_id,
            'journal_id_line_number': self._validate_line_number,
            'effective_date': self._validate_date,
            'entered_date': self._validate_date,
            'amount': self._validate_amount,
            'amount_debit': self._validate_amount,
            'amount_credit': self._validate_amount,
            'amount_currency': self._validate_currency,
            'amount_credit_debit_indicator': self._validate_indicator,
            'fiscal_year': self._validate_year,
            'period': self._validate_period,
            'gl_account_number': self._validate_account_number,
            'je_header_description': self._validate_description,
            'je_line_description': self._validate_description,
            'company_code': self._validate_code,
            'business_unit_code': self._validate_code,
            'document_type': self._validate_code,
            'entered_by': self._validate_user,
            'reversal_indicator': self._validate_boolean,
            'ledger': self._validate_text
        }
    
    def find_field_mapping(self, field_name: str, erp_system: str = None, 
                          sample_data: pd.Series = None) -> Optional[Tuple[str, float]]:
        """
        Encuentra el mapeo de campo más probable usando análisis de contenido avanzado
        y aprendizaje de patrones
        """
        self.mapping_stats['total_mappings_requested'] += 1
        
        # Verificar cache primero
        cache_key = (field_name, erp_system or 'None', str(hash(tuple(sample_data.dropna().head(3).tolist()))) if sample_data is not None else 'None')
        if cache_key in self._mapping_cache:
            self.mapping_stats['cache_hits'] += 1
            return self._mapping_cache[cache_key]
        
        # Si no hay datos de muestra, hacer análisis básico solo con sinónimos
        if sample_data is None or len(sample_data.dropna()) == 0:
            result = self._basic_synonym_matching(field_name, erp_system)
        else:
            result = self._enhanced_content_analysis(field_name, erp_system, sample_data)
        
        # Guardar en cache
        self._mapping_cache[cache_key] = result
        
        return result
    
    def _enhanced_content_analysis(self, field_name: str, erp_system: str, 
                                 sample_data: pd.Series) -> Optional[Tuple[str, float]]:
        """Análisis de contenido mejorado con aprendizaje de patrones"""
        print(f"\n🔍 ENHANCED ANALYSIS for '{field_name}':")
        print(f"  Sample data: {sample_data.dropna().head(3).tolist()}")
        
        field_definitions = self.field_loader.get_field_definitions()
        all_candidates = []
        
        # Analizar cada tipo de campo contra el contenido
        for field_type, field_def in field_definitions.items():
            # 1. Score de validación de contenido especializada
            content_score = self._analyze_content_coherence(field_type, sample_data)
            
            # 2. Score de patrones aprendidos
            pattern_score = self.pattern_learner.calculate_pattern_match_score(field_type, sample_data)
            
            # 3. Score de coincidencia de sinónimos
            synonym_score = self._get_synonym_score(field_name, field_def, erp_system)
            
            # Combinar scores con pesos optimizados
            if content_score > 0.7 or pattern_score > 0.7:
                # Contenido o patrones muy fuertes - priorizarlos
                total_score = content_score * 0.5 + pattern_score * 0.3 + synonym_score * 0.2
            elif content_score > 0.4 or pattern_score > 0.4:
                # Contenido o patrones moderados
                total_score = content_score * 0.4 + pattern_score * 0.3 + synonym_score * 0.3
            else:
                # Contenido débil - más peso a sinónimos
                total_score = content_score * 0.2 + pattern_score * 0.2 + synonym_score * 0.6
            
            if total_score > 0.15:  # Umbral mínimo
                all_candidates.append({
                    'field_type': field_type,
                    'field_name': field_def.name,
                    'total_score': total_score,
                    'content_score': content_score,
                    'pattern_score': pattern_score,
                    'synonym_score': synonym_score
                })
                
                print(f"  • {field_type}: content={content_score:.3f}, "
                      f"pattern={pattern_score:.3f}, synonym={synonym_score:.3f}, "
                      f"total={total_score:.3f}")
        
        # Ordenar candidatos por score total
        all_candidates.sort(key=lambda x: x['total_score'], reverse=True)
        
        if not all_candidates:
            print(f"  ❌ No candidates found")
            self.mapping_stats['failed_mappings'] += 1
            return None
        
        # Aplicar reglas especiales de corrección
        result = self._apply_correction_rules(all_candidates, sample_data)
        if result:
            return result
        
        # Verificar conflictos y resolverlos
        best_candidate = all_candidates[0]
        
        if len(all_candidates) > 1:
            second_best = all_candidates[1]
            score_diff = best_candidate['total_score'] - second_best['total_score']
            
            if score_diff < 0.15:  # Scores muy similares
                print(f"  ⚠️ Close match: {best_candidate['field_type']} ({best_candidate['total_score']:.3f}) "
                      f"vs {second_best['field_type']} ({second_best['total_score']:.3f})")
                self.mapping_stats['conflicts_resolved'] += 1
                
                # Resolver conflicto con reglas específicas
                resolved = self._resolve_conflict(best_candidate, second_best, sample_data)
                if resolved:
                    best_candidate = resolved
        
        # Retornar el mejor candidato
        winner = best_candidate['field_type']
        confidence = best_candidate['total_score']
        
        print(f"  ✅ WINNER: {winner} (confidence: {confidence:.3f})")
        
        self.mapping_stats['successful_mappings'] += 1
        self.mapping_stats['content_analysis_used'] += 1
        
        return (winner, confidence)
    
    def _apply_correction_rules(self, candidates: List[Dict], sample_data: pd.Series) -> Optional[Tuple[str, float]]:
        """Aplica reglas específicas de corrección para casos problemáticos"""
        
        # Regla 1: Si detecta fecha, NO puede ser journal_id
        if self._is_date_data(sample_data):
            date_candidates = [c for c in candidates if 'date' in c['field_type']]
            if date_candidates:
                best_date = max(date_candidates, key=lambda x: x['total_score'])
                print(f"  🔧 CORRECTION: Date detected -> {best_date['field_type']}")
                return (best_date['field_type'], best_date['total_score'])
        
        # Regla 2: Si detecta importe, NO puede ser currency o indicator
        if self._is_amount_data(sample_data):
            amount_candidates = [c for c in candidates if c['field_type'] in ['amount', 'amount_debit', 'amount_credit']]
            if amount_candidates:
                best_amount = max(amount_candidates, key=lambda x: x['total_score'])
                print(f"  🔧 CORRECTION: Amount detected -> {best_amount['field_type']}")
                return (best_amount['field_type'], best_amount['total_score'])
        
        # Regla 3: Si detecta texto descriptivo, NO puede ser amount
        if self._is_description_data(sample_data):
            desc_candidates = [c for c in candidates if 'description' in c['field_type']]
            if desc_candidates:
                best_desc = max(desc_candidates, key=lambda x: x['total_score'])
                print(f"  🔧 CORRECTION: Description detected -> {best_desc['field_type']}")
                return (best_desc['field_type'], best_desc['total_score'])
        
        return None
    
    def _resolve_conflict(self, candidate1: Dict, candidate2: Dict, sample_data: pd.Series) -> Optional[Dict]:
        """Resuelve conflictos entre candidatos con scores similares"""
        
        # Priorizar campos que tengan mejor validación de contenido específica
        val1 = self._validate_field_content(candidate1['field_type'], sample_data)
        val2 = self._validate_field_content(candidate2['field_type'], sample_data)
        
        if val1 > val2 + 0.2:
            return candidate1
        elif val2 > val1 + 0.2:
            return candidate2
        
        # Si siguen empatados, priorizar por tipo de campo (fechas > importes > textos)
        priority_order = {
            'effective_date': 10, 'entered_date': 9,
            'amount': 8, 'amount_debit': 7, 'amount_credit': 6,
            'journal_id': 5, 'journal_id_line_number': 4,
            'gl_account_number': 3,
            'je_header_description': 2, 'je_line_description': 1
        }
        
        prio1 = priority_order.get(candidate1['field_type'], 0)
        prio2 = priority_order.get(candidate2['field_type'], 0)
        
        return candidate1 if prio1 > prio2 else candidate2
    
    def _analyze_content_coherence(self, field_type: str, sample_data: pd.Series) -> float:
        """
        Análisis de coherencia de contenido mejorado con validadores especializados
        """
        if field_type not in self.field_validators:
            return 0.0
        
        # Verificar cache
        cache_key = (field_type, str(hash(tuple(sample_data.dropna().head(5).tolist()))))
        if cache_key in self._content_analysis_cache:
            return self._content_analysis_cache[cache_key]
        
        # Ejecutar validador especializado
        try:
            score = self.field_validators[field_type](sample_data)
            score = max(0.0, min(1.0, score))  # Normalizar entre 0 y 1
            
            # Guardar en cache
            self._content_analysis_cache[cache_key] = score
            
            return score
            
        except Exception as e:
            logger.warning(f"Error in content coherence analysis for {field_type}: {e}")
            return 0.0
    
    def _validate_field_content(self, field_type: str, sample_data: pd.Series) -> float:
        """Validación específica de contenido por tipo de campo"""
        return self._analyze_content_coherence(field_type, sample_data)
    
    # ====== VALIDADORES ESPECIALIZADOS ======
    
    def _validate_journal_id(self, series: pd.Series) -> float:
        """Valida identificadores de asientos contables"""
        clean_series = series.dropna().astype(str)
        if len(clean_series) == 0:
            return 0.0
        
        valid_count = 0
        total_count = len(clean_series)
        
        for value in clean_series:
            value = str(value).strip()
            
            # Patrones para journal IDs
            patterns = [
                r'^\d{6,15}$',                    # Números largos
                r'^JE\d{6,12}$',                  # JE + números
                r'^AST\d{4,10}$',                 # AST + números
                r'^[A-Z]{2,4}\d{6,12}$',         # Letras + números
                r'^\d{4}[A-Z]{2,4}\d{4,8}$',     # Año + letras + números
                r'^[A-Z0-9]{8,20}$'              # Alfanumérico general
            ]
            
            # Verificar patrones
            if any(re.match(pattern, value.upper()) for pattern in patterns):
                valid_count += 1
            elif len(value) >= 6 and value.isdigit():
                valid_count += 0.8  # Números largos probablemente válidos
            elif len(value) >= 4 and re.match(r'^[A-Z0-9]+$', value.upper()):
                valid_count += 0.6  # Alfanumérico general
            
            # PENALIZAR si parece fecha
            if self._is_date_like(value):
                valid_count -= 0.5
        
        return max(0.0, min(valid_count / total_count, 1.0))
    
    def _validate_line_number(self, series: pd.Series) -> float:
        """Valida números de línea de asiento"""
        clean_series = series.dropna()
        if len(clean_series) == 0:
            return 0.0
        
        valid_count = 0
        total_count = len(clean_series)
        
        for value in clean_series:
            try:
                num_value = float(str(value).replace(',', '.'))
                if 1 <= num_value <= 9999 and num_value == int(num_value):
                    valid_count += 1
                elif str(value).strip().isdigit():
                    valid_count += 0.8
            except:
                # Si parece fecha, penalizar fuertemente
                if self._is_date_like(str(value)):
                    valid_count -= 0.3
        
        return max(0.0, min(valid_count / total_count, 1.0))
    
    def _validate_date(self, series: pd.Series) -> float:
        """Valida fechas"""
        clean_series = series.dropna().astype(str)
        if len(clean_series) == 0:
            return 0.0
        
        valid_count = 0
        total_count = len(clean_series)
        
        for value in clean_series:
            if self._is_date_like(value):
                valid_count += 1
        
        return valid_count / total_count
    
    def _validate_amount(self, series: pd.Series) -> float:
        """Valida importes monetarios"""
        clean_series = series.dropna().astype(str)
        if len(clean_series) == 0:
            return 0.0
        
        valid_count = 0
        total_count = len(clean_series)
        
        for value in clean_series:
            value_str = str(value).strip()
            
            # Patrones de importes
            amount_patterns = [
                r'^-?\d{1,3}(\.\d{3})*,\d{2}$',      # 1.234.567,89
                r'^-?\d{1,3}(,\d{3})*\.\d{2}$',      # 1,234,567.89
                r'^-?\d+[,\.]\d{1,4}$',              # 1234,56 o 1234.56
                r'^-?\d+$',                          # 1234
                r'^-?\d+\.\d+$',                     # 1234.56
                r'^-?\d+,\d+$'                       # 1234,56
            ]
            
            if any(re.match(pattern, value_str) for pattern in amount_patterns):
                valid_count += 1
            elif self._is_numeric(value_str):
                valid_count += 0.7
            
            # PENALIZAR si parece descripción
            if len(value_str) > 2 and any(c.isalpha() for c in value_str):
                valid_count -= 0.3
        
        return max(0.0, min(valid_count / total_count, 1.0))
    
    def _validate_currency(self, series: pd.Series) -> float:
        """Valida códigos de moneda"""
        clean_series = series.dropna().astype(str)
        if len(clean_series) == 0:
            return 0.0
        
        valid_count = 0
        total_count = len(clean_series)
        
        common_currencies = {'EUR', 'USD', 'GBP', 'JPY', 'CHF', 'CAD', 'AUD', 'CNY', 'MXN'}
        
        for value in clean_series:
            value_str = str(value).strip().upper()
            
            if value_str in common_currencies:
                valid_count += 1
            elif len(value_str) == 3 and value_str.isalpha():
                valid_count += 0.7
            elif value_str in ['€', '$', '£', '¥']:
                valid_count += 0.8
            
            # PENALIZAR si parece importe numérico
            if self._is_numeric(value_str):
                valid_count -= 0.5
        
        return max(0.0, min(valid_count / total_count, 1.0))
    
    def _validate_indicator(self, series: pd.Series) -> float:
        """Valida indicadores debe/haber"""
        clean_series = series.dropna().astype(str)
        if len(clean_series) == 0:
            return 0.0
        
        valid_count = 0
        total_count = len(clean_series)
        
        valid_indicators = {'D', 'C', 'H', 'DEBE', 'HABER', 'DEBIT', 'CREDIT', 'DR', 'CR', '1', '0', '-1'}
        
        for value in clean_series:
            value_str = str(value).strip().upper()
            
            if value_str in valid_indicators:
                valid_count += 1
            elif len(value_str) == 1 and value_str in 'DCHX+-':
                valid_count += 0.8
            
            # PENALIZAR fuertemente si parece importe numérico grande
            if self._is_numeric(value_str) and abs(float(value_str.replace(',', '.'))) > 10:
                valid_count -= 0.8
        
        return max(0.0, min(valid_count / total_count, 1.0))
    
    def _validate_year(self, series: pd.Series) -> float:
        """Valida años fiscales"""
        clean_series = series.dropna()
        if len(clean_series) == 0:
            return 0.0
        
        valid_count = 0
        total_count = len(clean_series)
        
        for value in clean_series:
            try:
                year_value = int(float(str(value)))
                if 1900 <= year_value <= 2100:
                    valid_count += 1
                elif 10 <= year_value <= 99:  # Años en formato de 2 dígitos
                    valid_count += 0.7
            except:
                pass
        
        return valid_count / total_count
    
    def _validate_period(self, series: pd.Series) -> float:
        """Valida períodos contables"""
        clean_series = series.dropna()
        if len(clean_series) == 0:
            return 0.0
        
        valid_count = 0
        total_count = len(clean_series)
        
        for value in clean_series:
            try:
                period_value = int(float(str(value)))
                if 1 <= period_value <= 12:
                    valid_count += 1
                elif 1 <= period_value <= 53:  # Semanas
                    valid_count += 0.6
            except:
                value_str = str(value).upper()
                # Períodos en formato texto
                if any(month in value_str for month in ['ENE', 'FEB', 'MAR', 'ABR', 'MAY', 'JUN',
                                                      'JUL', 'AGO', 'SEP', 'OCT', 'NOV', 'DIC']):
                    valid_count += 0.8
        
        return valid_count / total_count
    
    def _validate_account_number(self, series: pd.Series) -> float:
        """Valida números de cuenta contable"""
        clean_series = series.dropna().astype(str)
        if len(clean_series) == 0:
            return 0.0
        
        valid_count = 0
        total_count = len(clean_series)
        
        for value in clean_series:
            value_str = str(value).strip()
            
            # Patrones de cuentas contables
            if re.match(r'^\d{3,10}$', value_str):  # Solo números, 3-10 dígitos
                valid_count += 1
            elif re.match(r'^\d{3,6}\.\d{2,4}$', value_str):  # Con punto
                valid_count += 1
            elif re.match(r'^\d{3,6}-\d{2,4}$', value_str):  # Con guión
                valid_count += 1
            elif len(value_str) >= 3 and value_str.isdigit():
                valid_count += 0.8
            
            # PENALIZAR si parece período (números muy pequeños)
            try:
                num_val = int(value_str)
                if 1 <= num_val <= 12:
                    valid_count -= 0.3
            except:
                pass
        
        return max(0.0, min(valid_count / total_count, 1.0))
    
    def _validate_description(self, series: pd.Series) -> float:
        """Valida descripciones de texto"""
        clean_series = series.dropna().astype(str)
        if len(clean_series) == 0:
            return 0.0
        
        valid_count = 0
        total_count = len(clean_series)
        
        for value in clean_series:
            value_str = str(value).strip()
            
            # Debe contener texto y tener longitud razonable
            if len(value_str) >= 3:
                has_letters = any(c.isalpha() for c in value_str)
                has_spaces_or_words = ' ' in value_str or len(value_str) > 10
                
                if has_letters and has_spaces_or_words:
                    valid_count += 1
                elif has_letters:
                    valid_count += 0.6
            
            # PENALIZAR fuertemente si es completamente numérico
            if value_str.replace('.', '').replace(',', '').replace('-', '').isdigit():
                valid_count -= 0.5
        
        return max(0.0, min(valid_count / total_count, 1.0))
    
    def _validate_code(self, series: pd.Series) -> float:
        """Valida códigos (empresa, unidad de negocio, documento)"""
        clean_series = series.dropna().astype(str)
        if len(clean_series) == 0:
            return 0.0
        
        valid_count = 0
        total_count = len(clean_series)
        
        for value in clean_series:
            value_str = str(value).strip()
            
            # Códigos suelen ser cortos y alfanuméricos
            if 2 <= len(value_str) <= 10:
                if re.match(r'^[A-Z0-9]+$', value_str.upper()):
                    valid_count += 1
                elif value_str.replace('-', '').replace('_', '').isalnum():
                    valid_count += 0.8
        
        return valid_count / total_count
    
    def _validate_user(self, series: pd.Series) -> float:
        """Valida nombres de usuario"""
        clean_series = series.dropna().astype(str)
        if len(clean_series) == 0:
            return 0.0
        
        valid_count = 0
        total_count = len(clean_series)
        
        for value in clean_series:
            value_str = str(value).strip()
            
            # Usuarios suelen ser códigos o nombres
            if len(value_str) >= 2:
                if re.match(r'^[A-Za-z][A-Za-z0-9._-]*$', value_str):
                    valid_count += 1
                elif any(c.isalpha() for c in value_str):
                    valid_count += 0.6
        
        return valid_count / total_count
    
    def _validate_boolean(self, series: pd.Series) -> float:
        """Valida indicadores booleanos"""
        clean_series = series.dropna().astype(str)
        if len(clean_series) == 0:
            return 0.0
        
        valid_count = 0
        total_count = len(clean_series)
        
        boolean_values = {'Y', 'N', 'S', 'TRUE', 'FALSE', '1', '0', 'SI', 'NO', 'YES'}
        
        for value in clean_series:
            value_str = str(value).strip().upper()
            
            if value_str in boolean_values:
                valid_count += 1
            elif len(value_str) == 1 and value_str in 'YNSTF10':
                valid_count += 0.8
        
        return valid_count / total_count
    
    def _validate_text(self, series: pd.Series) -> float:
        """Validación genérica para campos de texto"""
        clean_series = series.dropna().astype(str)
        if len(clean_series) == 0:
            return 0.0
        
        valid_count = 0
        total_count = len(clean_series)
        
        for value in clean_series:
            value_str = str(value).strip()
            
            if len(value_str) >= 1:
                valid_count += 1
        
        return valid_count / total_count
    
    # ====== MÉTODOS DE UTILIDAD ======
    
    def _is_date_data(self, series: pd.Series) -> bool:
        """Verifica si los datos parecen fechas"""
        clean_series = series.dropna().astype(str).head(5)
        date_count = sum(1 for value in clean_series if self._is_date_like(value))
        return date_count / len(clean_series) > 0.6 if len(clean_series) > 0 else False
    
    def _is_amount_data(self, series: pd.Series) -> bool:
        """Verifica si los datos parecen importes"""
        clean_series = series.dropna().astype(str).head(5)
        amount_count = sum(1 for value in clean_series if self._is_numeric(value) and not self._is_date_like(value))
        return amount_count / len(clean_series) > 0.6 if len(clean_series) > 0 else False
    
    def _is_description_data(self, series: pd.Series) -> bool:
        """Verifica si los datos parecen descripciones"""
        clean_series = series.dropna().astype(str).head(5)
        desc_count = 0
        
        for value in clean_series:
            if len(value) > 5 and any(c.isalpha() for c in value):
                desc_count += 1
        
        return desc_count / len(clean_series) > 0.6 if len(clean_series) > 0 else False
    
    def _is_date_like(self, value: str) -> bool:
        """Verifica si un valor parece una fecha"""
        value_str = str(value).strip()
        
        date_patterns = [
            r'^\d{1,2}[/\-\.]\d{1,2}[/\-\.]\d{2,4}$',
            r'^\d{4}[/\-\.]\d{1,2}[/\-\.]\d{1,2}$',
            r'^\d{1,2}-[a-zA-Z]{3}-\d{2,4}$',
            r'^\d{2,4}[/\-\.]\d{1,2}[/\-\.]\d{1,2}$'
        ]
        
        return any(re.match(pattern, value_str) for pattern in date_patterns)
    
    def _is_numeric(self, value: str) -> bool:
        """Verifica si un valor es numérico"""
        try:
            # Limpiar formato de número
            clean_value = str(value).replace(',', '.').replace(' ', '')
            # Permitir un signo negativo al inicio
            if clean_value.startswith('-'):
                clean_value = clean_value[1:]
            
            float(clean_value)
            return True
        except:
            return False
    
    def _get_synonym_score(self, field_name: str, field_def: DynamicFieldDefinition, 
                          erp_system: str = None) -> float:
        """Calcula score de coincidencia con sinónimos"""
        # Implementación básica de sinónimos (mantenida del código original)
        normalized_name = self._normalize_field_name(field_name)
        
        if not hasattr(field_def, 'synonyms_by_erp'):
            return 0.0
        
        best_score = 0.0
        
        # Buscar en sinónimos específicos del ERP
        if erp_system and erp_system in field_def.synonyms_by_erp:
            for synonym_data in field_def.synonyms_by_erp[erp_system]:
                synonym_normalized = self._normalize_field_name(synonym_data.name)
                
                if normalized_name == synonym_normalized:
                    score = 0.9 * synonym_data.confidence_boost
                    best_score = max(best_score, score)
                elif synonym_normalized in normalized_name or normalized_name in synonym_normalized:
                    score = 0.7 * synonym_data.confidence_boost
                    best_score = max(best_score, score)
        
        # Buscar en otros ERPs si no se encontró en el específico
        if best_score < 0.3:
            for erp, synonyms in field_def.synonyms_by_erp.items():
                if erp == erp_system:
                    continue
                    
                for synonym_data in synonyms:
                    synonym_normalized = self._normalize_field_name(synonym_data.name)
                    
                    if normalized_name == synonym_normalized:
                        score = 0.8 * synonym_data.confidence_boost
                        best_score = max(best_score, score)
                    elif synonym_normalized in normalized_name or normalized_name in synonym_normalized:
                        score = 0.6 * synonym_data.confidence_boost
                        best_score = max(best_score, score)
        
        return best_score
    
    def _normalize_field_name(self, field_name: str) -> str:
        """Normaliza nombres de campos para comparación"""
        if field_name in self._normalization_cache:
            return self._normalization_cache[field_name]
        
        # Proceso de normalización
        normalized = field_name.lower().strip()
        
        # Eliminar acentos
        normalized = unicodedata.normalize('NFD', normalized)
        normalized = ''.join(c for c in normalized if unicodedata.category(c) != 'Mn')
        
        # Eliminar caracteres especiales
        normalized = re.sub(r'[^a-z0-9\s]', ' ', normalized)
        
        # Reemplazar múltiples espacios por uno solo
        normalized = re.sub(r'\s+', ' ', normalized).strip()
        
        # Eliminar espacios para comparación exacta
        normalized = normalized.replace(' ', '')
        
        # Traducir términos comunes
        for foreign_word, spanish_word in self.translation_map.items():
            if foreign_word in normalized:
                normalized = normalized.replace(foreign_word, spanish_word)
        
        self._normalization_cache[field_name] = normalized
        return normalized
    
    def _basic_synonym_matching(self, field_name: str, erp_system: str = None) -> Optional[Tuple[str, float]]:
        """Análisis básico basado solo en sinónimos (fallback)"""
        normalized_name = self._normalize_field_name(field_name)
        best_match = None
        best_confidence = 0.0
        
        field_definitions = self.field_loader.get_field_definitions()
        
        for field_type, field_def in field_definitions.items():
            confidence = self._get_synonym_score(field_name, field_def, erp_system)
            
            if confidence > best_confidence:
                best_confidence = confidence
                best_match = field_type
        
        if best_match and best_confidence > 0.3:
            self.mapping_stats['successful_mappings'] += 1
            return (best_match, best_confidence)
        
        self.mapping_stats['failed_mappings'] += 1
        return None
    
    def learn_from_training(self, field_type: str, field_name: str, sample_data: pd.Series, 
                           user_confirmed: bool = True):
        """Aprende patrones de un entrenamiento específico"""
        if sample_data is not None and len(sample_data.dropna()) > 0:
            self.pattern_learner.learn_pattern(field_type, sample_data, user_confirmed)
            self.mapping_stats['pattern_learning_used'] += 1
            print(f"  📚 Pattern learned for {field_type} from '{field_name}'")
    
    def get_mapping_statistics(self) -> Dict:
        """Retorna estadísticas de mapeo"""
        stats = self.mapping_stats.copy()
        stats['success_rate'] = stats['successful_mappings'] / max(stats['total_mappings_requested'], 1) * 100
        stats['cache_hit_rate'] = stats['cache_hits'] / max(stats['total_mappings_requested'], 1) * 100
        return stats
    
    def clear_cache(self):
        """Limpia caches para liberar memoria"""
        self._mapping_cache.clear()
        self._normalization_cache.clear()
        self._content_analysis_cache.clear()
        print("✓ Field mapper caches cleared")


# Mantener compatibilidad con el código existente
FieldMapper = EnhancedFieldMapper